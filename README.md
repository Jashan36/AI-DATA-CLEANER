# ğŸ”§ AI Data Cleaner & Web Scraper

A fully automated Data Quality & Web-Scrape Summarizer that autonomously cleans tabular data and turns web pages into clear, human-readable summaries with traceable structure. Built with policy-driven cleaning, comprehensive audit logging, and scalable processing.

## ğŸ¯ Core Features

### Policy-Driven Data Cleaning
- âœ… **Automatic Column Type Detection**: ID/Hash, Categorical, Free-text, Email, Phone, URL, Code, Currency, Rating, Date, Numeric, Boolean
- âœ… **Intelligent Strategy Selection**: Each column type gets appropriate cleaning strategy
- âœ… **Missing Value Handling**: Median for numeric, mode/unknown for categorical, forward-fill for dates
- âœ… **Outlier Management**: IQR-based capping with vectorized Polars operations
- âœ… **Standardization**: Unicode NFKC, case-folding, punctuation normalization

### Quality Gates & Validation
- âœ… **Great Expectations Integration**: Uniqueness, non-null, type expectations, range checks
- âœ… **Auto-fix Capabilities**: Safe transformations with quarantine for problematic data
- âœ… **Domain-Specific Rules**: Healthcare, finance, retail with specialized validation
- âœ… **Data Quality Reports**: Comprehensive DQR with before/after metrics

### Scalable Processing
- âœ… **Streaming Chunks**: Polars scan for memory-efficient processing
- âœ… **OOM Safety**: Configurable memory limits with garbage collection
- âœ… **Resume-able Runs**: Idempotent operations for large datasets
- âœ… **Intermediate Storage**: Parquet files for checkpointing

### Web Scraping & Summarization
- âœ… **Async Pipeline**: Concurrent requests with retry/backoff
- âœ… **Robots.txt Respect**: Ethical scraping with rate limiting
- âœ… **Structured Extraction**: About, Products, Team, Contact, Social, Pricing, FAQ, Jobs, Policies
- âœ… **NER + Rule-based**: spaCy entities with TextBlob fallback
- âœ… **Human-readable Summaries**: Concise bullet points (15-22 words) with source anchors

## ğŸš€ Enhanced Features

### Explainability & Audit
- âœ… **Comprehensive Audit Logs**: Policy decisions, before/after samples, confidence scores
- âœ… **Rule Tracking**: Every transformation logged with reasoning
- âœ… **Quarantine Management**: Problematic rows isolated with detailed reasons
- âœ… **Quality Metrics**: Before/after statistics, violation counts, auto-fix tracking

### Multiple Interfaces
- âœ… **Streamlit UI**: Upload/Connect, Quality Report, Explore, Web Summaries, Explain, Downloads
- âœ… **CLI Tool**: `clean-data` and `scrape` commands with full configuration
- âœ… **FastAPI**: REST endpoints for `/clean` and `/scrape` with async processing
- âœ… **Python API**: Direct integration for custom workflows

### Export Formats
- âœ… **Cleaned Data**: Parquet/CSV with preserved structure
- âœ… **Quarantined Data**: Problematic rows with reasons
- âœ… **Audit Logs**: JSON with complete transformation history
- âœ… **Quality Reports**: Markdown DQR with recommendations
- âœ… **Web Summaries**: Structured JSON + human-readable markdown

## ğŸ“Š Output Formats

### 1. Cleaned Dataset
- âœ… **Parquet/CSV**: High-performance format with preserved structure
- âœ… **Quality Assured**: Validated against domain-specific rules
- âœ… **Audit Trail**: Complete transformation history
- âœ… **Human-readable**: Maintains semantic richness

### 2. Comprehensive Audit Log (JSON)
```json
{
  "timestamp": "2024-01-15T10:30:00",
  "input_shape": [1000, 9],
  "output_shape": [950, 9],
  "quarantined_rows": 50,
  "quality_score": 0.95,
  "decisions": [
    {
      "column": "status",
      "column_type": "categorical_low",
      "strategy": "canonicalize_categorical",
      "confidence": 0.9,
      "rules_applied": ["case_folding", "synonym_merging"],
      "before_sample": ["Delivered", "DELIVERED", "delivered"],
      "after_sample": ["delivered", "delivered", "delivered"]
    }
  ]
}
```

### 3. Data Quality Report (Markdown)
- âœ… **Quality Score**: Overall data quality percentage
- âœ… **Violation Summary**: Errors, warnings, info counts
- âœ… **Auto-fixes Applied**: List of automatic corrections
- âœ… **Recommendations**: Actionable improvement suggestions

### 4. Web Scraping Results
- âœ… **Structured Cards**: JSON with extracted information
- âœ… **Human Summary**: Markdown with bullet points
- âœ… **Raw Text**: Cleaned HTML content
- âœ… **Entity Extraction**: NER results with confidence scores

## ğŸ› ï¸ Installation & Usage

### Prerequisites
- Python 3.8+ (for local installation)
- Docker & Docker Compose (for containerized deployment)
- 2GB+ RAM recommended
- 1GB+ storage for models

### ğŸ³ Docker Installation (Recommended)

#### Quick Start with Docker Compose
```bash
# Download the project
# Extract the project files to your desired location
cd AI-DATA-CLEANER

# Build and run with Docker Compose
docker-compose up --build

# Access the application at http://localhost:8501
```

#### Manual Docker Build
```bash
# Build the Docker image
docker build -t ai-data-cleaner .

# Run the container
docker run -p 8501:8501 -v $(pwd)/data:/app/data -v $(pwd)/output:/app/output ai-data-cleaner

# Access the application at http://localhost:8501
```

#### Optional: Jupyter Lab Service
```bash
# Run with Jupyter Lab for data exploration
docker-compose --profile jupyter up --build

# Access Jupyter Lab at http://localhost:8888
```

### ğŸ“¦ Local Installation

#### Traditional Installation
```bash
# Download the project
# Extract the project files to your desired location
cd AI-DATA-CLEANER

# Install dependencies
pip install -r requirements.txt

# Install optional NLP models
python -m spacy download en_core_web_sm
```

#### Streamlit Web App
```bash
streamlit run streamlit_app.py
```

#### CLI Tool
```bash
# Clean data
python cli.py clean-data --input data.csv --output results --domain healthcare

# Scrape URLs
python cli.py scrape --urls https://example.com https://another-site.com --output results
```

#### FastAPI Server
```bash
python api.py
# Access API at http://localhost:8000
```

#### Python API
```python
from core.data_cleaner import DataCleaner, CleaningConfig

# Initialize cleaner
config = CleaningConfig(
    chunk_size=10000,
    enable_quality_gates=True,
    enable_quarantine=True
)
cleaner = DataCleaner(config)

# Clean data file
result = cleaner.clean_file('messy_data.csv', 'output_dir', domain='healthcare')

if result.success:
    print(f"Cleaned {result.total_rows_cleaned} rows")
    print(f"Quarantined {result.total_rows_quarantined} rows")
    print(f"Processing time: {result.processing_time:.2f}s")
```

#### Example Usage
```bash
python example_usage.py
```

## ğŸ“ Project Structure
```
AI-DATA-CLEANER/
â”œâ”€â”€ core/                   # Core modules
â”‚   â”œâ”€â”€ cleaner_policy.py   # Policy-driven cleaning engine
â”‚   â”œâ”€â”€ data_cleaner.py     # Main data cleaner with streaming
â”‚   â”œâ”€â”€ quality_gates.py    # Great Expectations integration
â”‚   â””â”€â”€ web_scraper.py      # Async web scraping pipeline
â”œâ”€â”€ tests/                  # Test suite
â”‚   â”œâ”€â”€ test_cleaner_policy.py
â”‚   â”œâ”€â”€ test_quality_gates.py
â”‚   â”œâ”€â”€ test_data_cleaner.py
â”‚   â””â”€â”€ test_web_scraper.py
â”œâ”€â”€ streamlit_app.py        # Streamlit web interface
â”œâ”€â”€ cli.py                  # Command-line interface
â”œâ”€â”€ api.py                  # FastAPI REST endpoints
â”œâ”€â”€ test_core.py           # Basic functionality tests
â”œâ”€â”€ run_tests.py           # Test runner
â”œâ”€â”€ requirements.txt       # Dependencies
â”œâ”€â”€ POLICIES.md            # Cleaning policies documentation
â”œâ”€â”€ API.md                 # API documentation
â”œâ”€â”€ OPERATIONS.md          # Operations guide
â”œâ”€â”€ MAPPINGS.md            # Categorical mappings
â”œâ”€â”€ Dockerfile             # Docker configuration
â”œâ”€â”€ docker-compose.yml     # Docker Compose configuration
â””â”€â”€ README.md              # This file
```

## ğŸ¯ Example Workflow

### Input: Messy Healthcare Dataset
```csv
Patient ID,Age,Gender,Symptoms,Diagnosis,Visit Date
P001,25,Male,Fever,Hypertension,2024-01-15
P002,30,FEMALE,FEVER,HYPERTENSION,15/01/2024
P003,45,M,Headache,Diabetes,Jan 15, 2024
P004,60,Female,COUGH,DIABETES,2024-01-15
P005,35,Other,Nausea,Asthma,15-01-2024
```

### Output: Cleaned Dataset
```csv
patient_id,age,gender,symptoms,diagnosis,visit_date
P001,25,male,fever,hypertension,2024-01-15
P002,30,female,fever,hypertension,2024-01-15
P003,45,male,headache,diabetes,2024-01-15
P004,60,female,cough,diabetes,2024-01-15
P005,35,other,nausea,asthma,2024-01-15
```

### Audit Log Summary
```json
{
  "timestamp": "2024-01-15T10:30:00",
  "input_shape": [5, 6],
  "output_shape": [5, 6],
  "quarantined_rows": 0,
  "quality_score": 0.95,
  "decisions": [
    {
      "column": "gender",
      "strategy": "canonicalize_categorical",
      "confidence": 0.9,
      "rules_applied": ["case_folding", "synonym_merging"]
    }
  ]
}
```

### Web Scraping Example
```bash
python cli.py scrape --urls https://example.com --output results
```

**Output:**
- `scrape_summary.md`: Human-readable summary with bullet points
- `scrape_cards.json`: Structured information cards
- `scrape_raw_text.txt`: Cleaned HTML content

## ğŸ”§ Behavioral Constraints

- âŒ **No Hallucination**: Never guess or invent values
- âœ… **Safe Imputation**: Replace unparseable values with NaN or "unknown"
- âœ… **Policy-Driven**: All decisions based on explicit rules and policies
- âœ… **Audit Trail**: Complete logging of all transformations with reasoning
- âœ… **Quality Gates**: Automatic validation with quarantine for problematic data

## ğŸ“ˆ Performance Features

- âš¡ **Polars Optimization**: Fast DataFrame operations with streaming
- ğŸš€ **Memory Efficient**: Handles datasets up to 1M+ rows with chunking
- ğŸ“Š **Async Processing**: Concurrent web scraping with rate limiting
- ğŸ”„ **Resume-able**: Idempotent operations for large datasets
- ğŸ›¡ï¸ **OOM Safe**: Configurable memory limits with garbage collection

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# Run all tests
python run_tests.py

# Run specific test modules
python -m pytest tests/test_cleaner_policy.py -v
python -m pytest tests/test_quality_gates.py -v
python -m pytest tests/test_data_cleaner.py -v
python -m pytest tests/test_web_scraper.py -v

# Run basic functionality test
python test_core.py
```

## ğŸ“š Documentation

- **[POLICIES.md](POLICIES.md)**: Detailed cleaning policies and decision rules
- **[API.md](API.md)**: REST API documentation with examples
- **[OPERATIONS.md](OPERATIONS.md)**: Production deployment and operations guide
- **[MAPPINGS.md](MAPPINGS.md)**: Categorical value mappings and canonicalization

## ğŸ¤ Contributing

This tool is designed for production use. Contributions should maintain high standards:
- Comprehensive audit logging and explainability
- Policy-driven decision making
- Performance optimization and scalability
- Error handling and validation
- Complete test coverage

## ğŸ“„ License
MIT License - See LICENSE file for details

---

**Built for Data Scientists & Engineers who demand production-grade data quality with complete transparency and audit trails.**
